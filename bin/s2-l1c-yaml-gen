
import io
import os
import logging
import math
import tempfile
import zipfile
import sys
from xml.etree import ElementTree
from datetime import datetime, timedelta
from os.path import join, basename
from subprocess import Popen, PIPE, check_output, CalledProcessError
from pathlib import Path

import click
from click_datetime import Datetime
from dateutil.parser import parse as date_parser

DEFAULT_S2_AOI = '/g/data/v10/eoancillarydata/S2_extent/S2_aoi.csv'
DEFAULT_S2_L1C = '/g/data/fj7/Copernicus/Sentinel-2/MSI/L1C'
DEFAULT_YAML_S2_L1C = '/g/data/v10/AGDCv2/indexed_datasets/cophub/s2_v2/s2_l1c_yamls'

_TODAY = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)


def _level1_dataset_path_iter(root_directory: Path, *find_options, subprocess_depth=1):
    """ Yields the path to level1 archives for sentinel2

    Function splits up filesystem recursion by first identifying files up to the
    subprocess_depth (in a subprocess) then separately runs find against all
    directories found at that depth.

    This is done in order to reduce the upper bound on memory consumed by
    the redirection of stdout.
    """
    def _run_find(level1_dir: Path, *find_options):
        find_options = find_options or []
        cmd = ['find', str(level1_dir)] + list(find_options) + ['-name', '*.zip', ]
        in_stream = io.TextIOWrapper(Popen(cmd, stdout=PIPE).stdout, encoding='utf-8')
        logging.info("calling %s", ' '.join(cmd))
        for level1_path in in_stream:
            yield Path(level1_path.strip())


    def _get_dirs_at_depth(root, depth=1):
        """ Returns a list of directories at a set depth from root"""

        def get_dirs_recurse(root, depth, max_depth=1):
            results = []
            for _path in root.iterdir():
                if _path.is_dir():
                    if depth == max_depth:
                        results.append(_path)
                    else:
                        try:
                            results.extend(get_dirs_recurse(_path, depth + 1, max_depth))
                        except OSError as e:
                            logging.exception(e)
            return results

        # Depth starts at one as it lists the files at the root
        return get_dirs_recurse(Path(root), 1, depth)

 
@click.group()
def cli():
    pass

@cli.command('s2-l1-yaml-gen')
@click.option('--level1-root',
              type=click.Path(exists=True, readable=True),
              default=DEFAULT_S2_L1C,
              help='directory to write yamls to')
@click.option('--output-dir',
              type=click.Path(exists=True, writable=True),
              callback=lambda ctx, param, value: Path(value),
              default=DEFAULT_YAML_S2_L1C,
              help='directory to write yamls to')
@click.option('--year-month', type=Datetime('%Y-%m'),
              default=(_TODAY.year, 1, 1),
              help='Year-month to generate yamls for. i.e. 2020-9')
@click.option('--dry-run', default=False, is_flag=True)
@click.option('--log-level', default="WARNING", type=str,
              help="Set a log level.  e.g. WARNING INFO")
def generate_level1(level1_root: Path, output_dir: Path, year_month: datetime,
                    dry_run: bool, log_level: str):
    click.echo(' '.join(sys.argv))
    try:
        logging.basicConfig(level=log_level)
    except:
        logging.basicConfig(level="WARNING")
        logging.warning("Log level defaulting to warning.")
    logging.info('this is an info level test')
    sys.exit()

    for level1_dataset in _level1_dataset_path_iter(Path(level1_root), *find_options):
        # Include the parent directories of the source file; yaml files are broken up into
        # 5 degree by 5 degree geographies like level1 datasets
        # There is no reason for this breakup
        yaml_output_dir = output_dir / '/'.join(level1_dataset.parts[-(1 + copy_parent_dir_count):-1])
        os.makedirs(yaml_output_dir, exist_ok=True)
        for i in range(retries):
            try:
                # This is to avoid skipping due to Dataset creation time being older than start date
                # Note if the difference is more than 7 days it will still skip
                start_date = start_date - timedelta(7)
                if dry_run:
                    click.echo(
                        'Processing: datasets: {}, outdir: {}, checksum: {}, start_date: {}'.format(
                            str(level1_dataset), str(yaml_output_dir), str(checksum), str(start_date)
                        )
                    )
                else:
                    logging.info('Processing archive: %s', str(level1_dataset))
                    _process_datasets(yaml_output_dir, (level1_dataset, ), checksum, start_date)
                break
            except Exception as e:
                logging.error('Issue processing archive: %s', str(level1_dataset))
                logging.exception(e)
        else:
            logging.error('Skipping: %s', str(level1_dataset))


if __name__ == '__main__':
    cli()
