#!/usr/bin/env python

"""
PBS submission scripts.
"""

import os
import re
import json
import logging
from os.path import join as pjoin, dirname, exists
from pathlib import Path
import subprocess
import uuid
import click
import math

from wagl.tiling import scatter


PBS_RESOURCES = ("""#!/bin/bash
#PBS -P {project}
#PBS -W umask=017
#PBS -q {queue}
#PBS -l walltime={walltime},mem={memory}GB,ncpus={ncpus},jobfs={jobfs}GB,other=pernodejobfs
#PBS -l wd
#PBS -l storage={filesystem_projects}
#PBS -me
{email}
""")

NODE_TEMPLATE = ("""{pbs_resources}
source {env}

{daemon}

luigi --module tesp.workflow ARDP --level1-list {scene_list} --workdir {outdir} --pkgdir {pkgdir} --workers {workers} --parallel-scheduling
""")

SUMMARY_TEMPLATE = ("""{pbs_resources}
#PBS -W depend=afterany:{jobids}

source {env}
batch_summary --indir {indir} --outdir {outdir}

# jq queries
# concatenate logs to enable querying on a single file
find -type f -name 'task-log.jsonl' | xargs cat >> batch-{batchid}-task-log.jsonl
find -type f -name 'status-log.jsonl' | xargs cat >> batch-{batchid}-status-log.jsonl

# summaries on success and failure records
jq 'select(.status == "success") | {{task, level1}}' batch-{batchid}-task-log.jsonl | jq --slurp 'unique_by(.level1, .task) | group_by(.task) | map({{task: .[0].task, count: length}})' > batch-{batchid}-success-task-summary.json
jq 'select(.status == "failure") | {{task, level1}}' batch-{batchid}-task-log.jsonl | jq --slurp 'unique_by(.level1, .task) | group_by(.task) | map({{task: .[0].task, count: length}})' > batch-{batchid}-failure-task-summary.json

# capture failures and report the exception, level1, task and granule id
jq 'select(.status == "failure") | {{level1, exception, task, granule: .params.granule}}' batch-{batchid}-task-log.jsonl | jq --slurp 'unique_by(.level1, .task, .granule)' > batch-{batchid}-exception-log.json

# get a listing of which level1-lists from which jobid's had 100% success (all scenes processed through to packaging)
jq 'select(.task == "ARDP") | {{task, level1_list: .params.level1_list, status}}' batch-{batchid}-task-log.jsonl | jq --slurp 'unique_by(.level1_list)' > batch-{batchid}-ardp-job-sucess.json

# get a listing of level1 datasets and the granule id that packaged successfully (Package is the last task to be done as defined in the luigi workflow) and report that "ard processing is complete"
jq 'select(.status == "success" and .task == "Package") | {{level1, granule: .params.granule, notification: "ard processing complete"}}' batch-{batchid}-task-log.jsonl | jq --slurp 'unique_by(.level1, .granule)' > batch-{batchid}-package-success.json

# compile a list of successfully packaged datasets (and their path) to pass over to indexing
jq 'select(.event == "packaged dataset") | .dataset_path' batch-{batchid}-status-log.jsonl | jq -sr 'unique | .[]' > batch-{batchid}-datasets-to-index.txt
""")

INDEXING_TEMPLATE = ("""{pbs_resources}
#PBS -W depend=afterany:{jobid}

source {env}

# indexing
cat batch-{batchid}-datasets-to-index.txt | parallel -j 47 -m -n 20 --line-buffer datacube dataset add --no-verify-lineage
""")


FMT1 = 'batchid-{batchid}'
FMT2 = 'jobid-{jobid}'
FMT3 = 'level1-scenes-{jobid}.txt'
FMT4 = 'jobid-{jobid}.bash'
FMT5 = 'batch-{batchid}-summary.bash'
FMT6 = 'batch-{batchid}-indexing.bash'
FMT7 = 'scratch/{f_project}+gdata/{f_project}'
DAEMON_FMT = 'luigid --background --logdir {}'

def _calc_nodes_req(granule_count, walltime, workers, hours_per_granule=1.5):
    """ Provides estimation of the number of nodes required to process granule count

    >>> _calc_nodes_req(400, '20:59', 28)
    2
    >>> _calc_nodes_req(800, '20:00', 28)
    3
    """

    hours, _, _ = [int(x) for x in walltime.split(':')]
    return int(math.ceil(float(hours_per_granule * granule_count) / (hours * workers)))


def _get_project_for_path(path: Path):
    """
    Get the NCI project used to store the given path, if any.
    >>> _get_project_for_path(Path('/g/data/v10/some/data/path.txt'))
    'v10'
    >>> _get_project_for_path(Path('/g/data4/fk4/some/data/path.txt'))
    'fk4'
    >>> _get_project_for_path(Path('/scratch/da82/path.txt'))
    'da82'
    >>> _get_project_for_path(Path('/tmp/other/data'))
    """
    posix_path = path.as_posix()
    if posix_path.startswith('/g/data'):
        return posix_path.split('/')[3]
    if posix_path.startswith('/scratch/'):
        return posix_path.split('/')[2]
    return None


def _filesystem_projects(level1_list: list,
                         env: str,
                         logdir: str,
                         workdir: str,
                         pkgdir: str):
    """
    Collect all the filesystem projects into a set.
    """
    fs_projects = {None}

    fs_projects.add(_get_project_for_path(Path(workdir)))
    fs_projects.add(_get_project_for_path(Path(logdir)))
    fs_projects.add(_get_project_for_path(Path(pkgdir)))
    fs_projects.add(_get_project_for_path(Path(env)))
    fs_projects.add(_get_project_for_path(Path(click.__file__)))

    with open(level1_list, 'r') as src:
        paths = [p.strip() for p in src.readlines()]

    for pathname in paths:
        fs_projects.add(_get_project_for_path(Path(pathname)))
    
    fs_projects.remove(None)

    return fs_projects


# pylint: disable=too-many-arguments
def _submit_multiple(scattered, env, batch_logdir, batch_outdir, pkgdir,
                     workers, pbs_resources, test):
    """Submit multiple PBS formatted jobs."""

    nci_job_ids = []

    # setup and submit each block of scenes for processing
    for block in scattered:
        jobid = uuid.uuid4().hex[0:6]
        jobdir = pjoin(batch_logdir, FMT2.format(jobid=jobid))
        job_outdir = pjoin(batch_outdir, FMT2.format(jobid=jobid))

        if not exists(jobdir):
            os.makedirs(jobdir)

        if not exists(job_outdir):
            os.makedirs(job_outdir)

        # write level1 data listing
        out_fname = pjoin(jobdir, FMT3.format(jobid=jobid))
        with open(out_fname, 'w') as src:
            src.writelines(block)

        pbs = NODE_TEMPLATE.format(pbs_resources=pbs_resources, env=env,
                                   daemon=DAEMON_FMT.format(jobdir),
                                   scene_list=out_fname, outdir=job_outdir,
                                   pkgdir=pkgdir, workers=workers)

        # write pbs script
        out_fname = pjoin(jobdir, FMT4.format(jobid=jobid))
        with open(out_fname, 'w') as src:
            src.write(pbs)

        if test:
            click.echo("Mocking... Submitting Job: {} ...Mocking".format(jobid))
            click.echo("qsub {}".format(out_fname))
            continue

        os.chdir(dirname(out_fname))
        click.echo("Submitting Job: {}".format(jobid))
        try:
            raw_output = subprocess.check_output(['qsub', out_fname])
        except subprocess.CalledProcessError as exc:
            logging.error('qsub failed with exit code %s', str(exc.returncode))
            logging.error(exc.output)
            raise

        if hasattr(raw_output, 'decode'):
            matches = re.match(r'^(?P<nci_job_id>\d+\.gadi-pbs)$', raw_output.decode('utf-8'))
            if matches:
                nci_job_ids.append(matches.groupdict()['nci_job_id'])

    # return a list of the nci job ids
    return nci_job_ids


def _submit_summary(indir, outdir, batch_id, pbs_resources, env, job_ids, test):
    """Summarise the jobs submitted within the batchjob."""
    jobids = ":".join([j.split('.')[0] for j in job_ids])
    pbs = SUMMARY_TEMPLATE.format(pbs_resources=pbs_resources, env=env,
                                  indir=indir, outdir=outdir, jobids=jobids,
                                  batchid=batch_id)

    out_fname = pjoin(indir, FMT5.format(batchid=batch_id))
    with open(out_fname, 'w') as src:
        src.write(pbs)

    if test:
        click.echo("Mocking... Submitting Summary Job for batch: {} ...Mocking".format(batch_id))
        click.echo("qsub {}".format(out_fname))
        return

    os.chdir(dirname(out_fname))
    click.echo("Submitting Summary Job for batch: {}".format(batch_id))
    try:
        raw_output = subprocess.check_output(['qsub', out_fname])
    except subprocess.CalledProcessError as exc:
        logging.error('qsub failed with exit code %s', str(exc.returncode))
        logging.error(exc.output)
        raise

    if hasattr(raw_output, 'decode'):
        matches = re.match(r'^(?P<nci_job_id>\d+\.gadi-pbs)$', raw_output.decode('utf-8'))
        if matches:
            job_id = matches.groupdict()['nci_job_id']

    return job_id


def _submit_index(indir, outdir, batch_id, pbs_resources, env, job_id, test):
    """Submit a job that adds datasets to a datacube index."""
    if job_id:
        jobid = job_id.split('.')[0]
    else:
        jobid = ''
    pbs = INDEXING_TEMPLATE.format(pbs_resources=pbs_resources, env=env,
                                   indir=indir, outdir=outdir, jobid=jobid,
                                   batchid=batch_id)

    out_fname = pjoin(indir, FMT6.format(batchid=batch_id))
    with open(out_fname, 'w') as src:
        src.write(pbs)

    if test:
        click.echo("Mocking... Submitting Indexing Job for batch: {} ...Mocking".format(batch_id))
        return

    os.chdir(dirname(out_fname))
    click.echo("Submitting Indexing Job for batch: {}".format(batch_id))
    try:
        raw_output = subprocess.check_output(['qsub', out_fname])
    except subprocess.CalledProcessError as exc:
        logging.error('qsub failed with exit code %s', str(exc.returncode))
        logging.error(exc.output)
        raise

    if hasattr(raw_output, 'decode'):
        matches = re.match(r'^(?P<nci_job_id>\d+\.gadi-pbs)$', raw_output.decode('utf-8'))
        if matches:
            job_id = matches.groupdict()['nci_job_id']

    return job_id


@click.command()
@click.option("--level1-list", type=click.Path(exists=True, readable=True),
              help="The input level1 scene list.")
@click.option("--workdir", type=click.Path(file_okay=False, writable=True),
              help="The base output working directory.")
@click.option("--logdir", type=click.Path(file_okay=False, writable=True),
              help="The base logging and scripts output directory.")
@click.option("--pkgdir", type=click.Path(file_okay=False, writable=True),
              help="The base output packaged directory.")
@click.option("--env", type=click.Path(exists=True, readable=True),
              help="Environment script to source.")
@click.option("--workers", type=click.IntRange(1, 48), default=30,
              help="The number of workers to request per node.")
@click.option("--nodes", default=0, help="The number of nodes to request.")
@click.option("--memory", default=192,
              help="The memory in GB to request per node.")
@click.option("--jobfs", default=50,
              help="The jobfs memory in GB to request per node.")
@click.option("--project", required=True, help="Project code to run under.")
@click.option("--queue", default='normal',
              help="Queue to submit the job into, eg normal, express.")
@click.option("--walltime", default="48:00:00",
              help="Job walltime in `hh:mm:ss` format.")
@click.option("--email", default="",
              help="Notification email address.")
@click.option("--index-datacube-env", type=click.Path(exists=True, readable=True),
              help="Datacube specific environment script to source.")
@click.option("--test", default=False, is_flag=True,
              help=("Test job execution (Don't submit the job to the "
                    "PBS queue)."))
# pylint: disable=too-many-arguments
def main(level1_list, workdir, logdir, pkgdir, env, workers, nodes, memory,
         jobfs, project, queue, walltime, email, index_datacube_env, test):
    """
    Equally partition a list of scenes across n nodes and submit
    n jobs into the PBS queue.
    """
    with open(level1_list, 'r') as src:
        scenes = src.readlines()
    if nodes == 0:
        nodes = _calc_nodes_req(len(scenes), walltime, workers)
    scattered = scatter(scenes, nodes)

    batchid = uuid.uuid4().hex[0:10]
    batch_logdir = pjoin(logdir, FMT1.format(batchid=batchid))
    batch_outdir = pjoin(workdir, FMT1.format(batchid=batchid))

    fs_projects = _filesystem_projects(level1_list, env, logdir, workdir,
                                       pkgdir)
    fsys_projects = '+'.join([FMT7.format(f_project=f) for f in fs_projects])

    # optionally set pbs email string
    pbs_resources = PBS_RESOURCES.format(project=project, queue=queue,
                                         walltime=walltime, memory=memory,
                                         ncpus=workers, jobfs=jobfs,
                                         filesystem_projects=fsys_projects,
                                         email=('#PBS -M ' + email) if email else "")

    if test:
        click.echo("Mocking... Submitting Batch: {} ...Mocking".format(batchid))
    else:
        click.echo("Submitting Batch: {}".format(batchid))

    click.echo("Executing Batch: {}".format(batchid))
    nci_job_ids = _submit_multiple(scattered, env, batch_logdir, batch_outdir,
                                   pkgdir, workers, pbs_resources, test)

    # job resources for batch summary
    pbs_resources = PBS_RESOURCES.format(project=project, queue='express',
                                         walltime="00:10:00", memory=6,
                                         ncpus=1, jobfs=2,
                                         filesystem_projects=''.join(fsys_projects),
                                         email=('#PBS -M ' + email) if email else "")

    job_id = _submit_summary(batch_logdir, batch_logdir, batchid,
                             pbs_resources, env, nci_job_ids, test)
    nci_job_ids.append(job_id)

    if index_datacube_env:
        pbs_resources = PBS_RESOURCES.format(project=project, queue='normal',
                                             walltime="00:30:00", memory=192,
                                             ncpus=48, jobfs=20,
                                             filesystem_projects=''.join(fsys_projects),
                                             email=('#PBS -M ' + email) if email else "")
        index_job_id = _submit_index(batch_logdir, batch_logdir, batchid,
                                     pbs_resources, index_datacube_env, job_id, test)
        nci_job_ids.append(index_job_id)

    job_details = {
        'ardpbs_batch_id': batchid,
        'nci_job_ids': nci_job_ids
    }

    # Enable the job details to be picked up by the calling process
    click.echo(json.dumps(job_details))


if __name__ == '__main__':
    main()
