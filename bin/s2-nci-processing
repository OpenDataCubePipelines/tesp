#!/usr/bin/env python3

import io
import os
import logging
import math
import tempfile
import zipfile
import sys
from xml.etree import ElementTree
from datetime import datetime
from os.path import join, basename
from subprocess import Popen, PIPE, check_output, CalledProcessError
from pathlib import Path

import click
from click_datetime import Datetime
from dateutil.parser import parse as date_parser

from wagl.acquisition import acquisitions
from tesp.workflow import Package
from eodatasets.prepare.s2_prepare_cophub_zip import _process_datasets


DEFAULT_S2_AOI = '/g/data/v10/eoancillarydata/S2_extent/S2_aoi.csv'
DEFAULT_S2_L1C = '/g/data/fj7/Copernicus/Sentinel-2/MSI/L1C'
DEFAULT_WORKDIR = '/g/data/if87/datacube/002/S2_MSI_ARD/workdir'
DEFAULT_LOGDIR = '/g/data/if87/datacube/002/S2_MSI_ARD/log_dir'
DEFAULT_PKGDIR = '/g/data/if87/datacube/002/S2_MSI_ARD/packaged'

_TODAY = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)


def get_archive_metadata(pathname: Path):
    """ Code to extract granule names from L1C Metadata, and processing baseline

    Logic has been ported from wagl.acquisition.__init__
    to avoid the overhead of caching the per measurement metadata
    methods in wagl.acquisition should be refactored to export
    this functionality

    returns a list of granule names
    """
    archive = zipfile.ZipFile(str(pathname))
    xmlfiles = [s for s in archive.namelist() if "MTD_MSIL1C.xml" in s]
    if not xmlfiles:
        pattern = basename(str(pathname).replace('PRD_MSIL1C', 'MTD_SAFL1C'))
        pattern = pattern.replace('.zip', '.xml')
        xmlfiles = [s for s in archive.namelist() if pattern in s]

    mtd_xml = archive.read(xmlfiles[0])
    xml_root = ElementTree.XML(mtd_xml)

    search_term = './*/Product_Info/Product_Organisation/Granule_List/Granules'
    grn_elements = xml_root.findall(search_term)

    # handling multi vs single granules + variants of each type
    if not grn_elements:
        grn_elements = xml_root.findall(search_term[:-1])

    if grn_elements[0].findtext('IMAGE_ID'):
        search_term = 'IMAGE_ID'
    else:
        search_term = 'IMAGE_FILE'

    # required to identify granule metadata in a multigranule archive
    # in the earlier l1c products
    processing_baseline = xml_root.findall('./*/Product_Info/PROCESSING_BASELINE')[0].text

    results = {}
    for granule in grn_elements:
        gran_id = granule.get('granuleIdentifier')
        if not pathname.suffix == '.zip':
            gran_path = str(pathname.parent.joinpath('GRANULE', gran_id, gran_id[:-7].replace('MSI', 'MTD') + '.xml'))
            root = ElementTree.parse(gran_path).getroot()
        else:
            xmlzipfiles = [s for s in archive.namelist() if 'MTD_TL.xml' in s]
            if not xmlzipfiles:
                pattern = gran_id.replace('MSI', 'MTD')
                pattern = pattern.replace('_N' + processing_baseline, '.xml')
                xmlzipfiles = [s for s in archive.namelist() if pattern in s]
            mtd_xml = archive.read(xmlzipfiles[0])
            root = ElementTree.XML(mtd_xml)
        sensing_time = root.findall('./*/SENSING_TIME')[0].text
        results[gran_id] = date_parser(sensing_time)

    return results


def _calc_nodes_req(granule_count, walltime, workers, hours_per_granule=1.5):
    """ Provides estimation of the number of nodes required to process granule count

    >>> _calc_nodes_req(400, '20:59', 28)
    2
    >>> _calc_nodes_req(800, '20:00', 28)
    3
    """

    hours, _, _ = [int(x) for x in walltime.split(':')]
    return int(math.ceil(float(hours_per_granule * granule_count) / (hours * workers)))


def _level1_dataset_path_iter(root_directory: Path, *find_options, subprocess_depth=1):
    """ Yields the path to level1 archives for sentinel2

    Function splits up filesystem recursion by first identifying files up to the
    subprocess_depth (in a subprocess) then separately runs find against all
    directories found at that depth.

    This is done in order to reduce the upper bound on memory consumed by
    the redirection of stdout.
    """
    def _run_find(level1_dir: Path, *find_options):
        find_options = find_options or []
        cmd = ['find', str(level1_dir)] + list(find_options) + ['-name', '*.zip', ]
        in_stream = io.TextIOWrapper(Popen(cmd, stdout=PIPE).stdout, encoding='utf-8')
        logging.info("calling %s", ' '.join(cmd))
        for level1_path in in_stream:
            yield Path(level1_path.strip())


    def _get_dirs_at_depth(root, depth=1):
        """ Returns a list of directories at a set depth from root"""

        def get_dirs_recurse(root, depth, max_depth=1):
            results = []
            for _path in root.iterdir():
                if _path.is_dir():
                    if depth == max_depth:
                        results.append(_path)
                    else:
                        try:
                            results.extend(get_dirs_recurse(_path, depth + 1, max_depth))
                        except OSError as e:
                            logging.exception(e)
            return results

        # Depth starts at one as it lists the files at the root
        return get_dirs_recurse(Path(root), 1, depth)


    # Handle base case
    yield from _run_find(root_directory, '-maxdepth', str(subprocess_depth), *find_options)
    # Handle seperate subprocess calls
    search_dirs = _get_dirs_at_depth(root_directory, subprocess_depth)
    for d in search_dirs:
        yield from _run_find(root_directory / d, *find_options)


def get_find_options(start_date, end_date, today=_TODAY):
    """ Returns the find options for subprocess call
    Assumes that range is defined: start_date <= file_modified_time < end_date

    >>> get_find_options(datetime(2018, 6, 30), datetime(2018, 7, 1), datetime(2018, 10, 15))
    ['-daystart', '-mtime', '-108', '-mtime', '+106']
    >>> get_find_options(datetime(2018, 6, 30), datetime(2018, 10, 15), datetime(2018, 10, 15))
    ['-daystart', '-mtime', '-108']

    """
    find_options = [
        '-daystart',
    ]
    min_in_days = (today - start_date).days + 1  # -1 since find is exclusive >
    find_options.extend(['-mtime', '-{}'.format(str(min_in_days))])
    max_in_days = (today - end_date).days
    if max_in_days:
        # Ignore upper bound if end_date is today
        find_options.extend(['-mtime', '+' + str(max_in_days)])

    return find_options


@click.group()
def cli():
    pass


@cli.command('process-level2')
@click.option('--level1-root', default=DEFAULT_S2_L1C, type=str,
              help="Folder containing Sentinel-2 level-1 datasets.")
@click.option('--s2-aoi', default=DEFAULT_S2_AOI, type=str,
              help="List of MGRD tiles of interest.")
@click.option('--start-date', type=Datetime(format='%Y-%m-%d'),
              help="Start of date range to process.")
@click.option('--end-date', type=Datetime(format='%Y-%m-%d'),
              help="End of date range to process.")
@click.option('--pkgdir', default=DEFAULT_PKGDIR, type=click.Path(file_okay=False, writable=True),
              help="The base output packaged directory.")
@click.option("--workdir", default=DEFAULT_WORKDIR, type=click.Path(file_okay=False, writable=True),
              help="The base output working directory.")
@click.option("--logdir", default=DEFAULT_LOGDIR, type=click.Path(file_okay=False, writable=True),
              help="The base logging and scripts output directory.")
@click.option("--env", type=click.Path(exists=True, readable=True),
              help="Environment script to source.")
@click.option("--workers", type=click.IntRange(1, 32), default=28,
              help="The number of workers to request per node.")
@click.option("--memory", default=256,
              help="The memory in GB to request per node.")
@click.option("--jobfs", default=60,
              help="The jobfs memory in GB to request per node.")
@click.option("--project", required=True, help="Project code to run under.")
@click.option("--queue", default='normalbw',
              help="Queue to submit the job into, e.g. normalbw, expressbw.")
@click.option("--walltime", default="48:00:00",
              help="Job walltime in `hh:mm:ss` format.")
@click.option("--email", default="",
              help="Notification email address.")
@click.option("--test", default=False, is_flag=True,
              help="Test job execution (Don't submit the job to the PBS queue).")
def process_level2(level1_root, s2_aoi, start_date, end_date, pkgdir, workdir, logdir, env,
                   workers, memory, jobfs, project, queue, walltime, email, test):

    click.echo(' '.join(sys.argv))

    logging.basicConfig(format='%(asctime)s %(levelname)s (%(pathname)s:%(lineno)s) %(message)s', level=logging.INFO)

    # Read area of interest list
    with open(s2_aoi) as csv:
        tile_ids = {'T' + tile.strip() for tile in csv}

    find_options = []

    if start_date or end_date:
        start_date = start_date or datetime(_TODAY.year, 1, 1)
        end_date = end_date or _TODAY

        find_options = get_find_options(start_date, end_date)

    def filter_granule_worker(out_stream):
        count = 0
        for level1_dataset in _level1_dataset_path_iter(Path(level1_root), *find_options):
            try:
                container = acquisitions(str(level1_dataset))
            except Exception as e:
                logging.warning('encountered unexpected error for %s: %s', str(level1_dataset), e)
                logging.exception(e)
                continue

            granule_md = get_archive_metadata(level1_dataset)

            for granule, sensing_date in granule_md.items():
                tile_id = granule.split('_')[-2]
                if tile_id not in tile_ids:
                    logging.debug('granule %s with MGRS tile ID %s outside AOI', granule, tile_id)
                    continue

                ymd = sensing_date.strftime('%Y-%m-%d')
                package = Package(
                    level1=str(level1_dataset),
                    workdir='',
                    granule=granule,
                    pkgdir=join(pkgdir, ymd)
                )
                if package.output().exists():
                    logging.debug('granule %s already processed', granule)
                    continue

                logging.debug('level1 dataset %s needs to be processed', level1_dataset)
                print(level1_dataset, file=out_stream)
                count += len(granule_md.keys())  # To handle multigranule files
                break
        return out_stream, count

    with tempfile.NamedTemporaryFile(mode="w+") as out_stream:
        _, granule_count = filter_granule_worker(out_stream)
        out_stream.flush()

        if granule_count == 0:
            logging.info("no granules to process.")
            return

        num_nodes = _calc_nodes_req(granule_count, walltime, workers)
        assert num_nodes > 0, "cannot ask for {} nodes".format(num_nodes)
        assert num_nodes <= 200, "number of nodes to request {} is too high".format(num_nodes)

        ard_cmd = ['ard_pbs', '--level1-list', out_stream.name, '--workdir', workdir,
                   '--logdir', logdir, '--pkgdir', pkgdir, '--env', env,
                   '--workers', str(workers), '--nodes', str(num_nodes), '--memory', str(memory),
                   '--jobfs', str(jobfs), '--project', project, '--queue', queue, '--walltime', walltime]

        if email:
            ard_cmd += ['--email', email]
        if test:
            ard_cmd += ['--test']

        logging.info("calling %s", ' '.join(ard_cmd))
        raw_output = None
        try:
            raw_output = check_output(ard_cmd)
        except CalledProcessError as exc:
            logging.error('ard_pbs failed with exit code %s', str(exc.returncode))
            logging.exception(exc.output)
            raise
        finally:
            if raw_output:
                # Echo job info, can be useful to track job submission
                click.echo(raw_output.decode('utf-8'))


@cli.command('generate-level1')
@click.option('--level1-root',
              type=click.Path(exists=True, readable=True),
              help='directory to write yamls to')
@click.option('--output-dir',
              type=click.Path(exists=True, writable=True),
              callback=lambda ctx, param, value: Path(value),
              help='directory to write yamls to')
@click.option('--start-date', type=Datetime('%Y-%m-%d'),
              default=(_TODAY.year, 1, 1),
              help='Start of date range for level 1 generation')
@click.option('--end-date', type=Datetime('%Y-%m-%d'),
              default=_TODAY,
              help='End of date range for level 1 generation')
@click.option('--copy-parent-dir-count', type=int, default=0)
@click.option('--retries', type=int, default=3)
@click.option('--checksum/--no-checksum', default=False)
@click.option('--dry-run', default=False, is_flag=True)
def generate_level1(level1_root: Path, output_dir: Path, start_date: datetime,
                    end_date: datetime, copy_parent_dir_count: int,
                    retries: int, checksum: bool, dry_run: bool):
    click.echo(' '.join(sys.argv))

    # run a find command to generate a list of level1 documents
    find_options = get_find_options(start_date, end_date)

    for level1_dataset in _level1_dataset_path_iter(Path(level1_root), *find_options):
        # Include the parent directories of the source file; yaml files are broken up into
        # 5 degree by 5 degree geographies like level1 datasets
        # There is no reason for this breakup
        yaml_output_dir = output_dir / '/'.join(level1_dataset.parts[-(1 + copy_parent_dir_count):-1])
        os.makedirs(yaml_output_dir, exist_ok=True)
        for i in range(retries):
            try:
                if dry_run:
                    click.echo(
                        'Processing: datasets: {}, outdir: {}, checksum: {}, start_date: {}'.format(
                            str(level1_dataset), str(yaml_output_dir), str(checksum), str(start_date)
                        )
                    )
                else:
                    _process_datasets(yaml_output_dir, (level1_dataset, ), checksum, start_date)
                break
            except Exception as e:
                logging.error('Issue processing archive: %s', str(level1_dataset))
                logging.exception(e)
        else:
            logging.error('Skipping: %s', str(level1_dataset))


if __name__ == '__main__':
    cli()
